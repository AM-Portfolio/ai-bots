# ğŸ¤– LLM Provider Auto-Detection & Smart Fallback

## ğŸ¯ Overview

The LLM provider system now intelligently detects configured providers from your `.env` file and automatically handles fallback logic based on what's available.

---

## ğŸ” How It Works

### **3 Operating Modes**

#### **1. No Providers Configured** âŒ
**Behavior**: Raises an error immediately

```bash
# .env (missing credentials)
# AZURE_OPENAI_ENDPOINT=
# AZURE_OPENAI_API_KEY=
# TOGETHER_API_KEY=
```

**Result**:
```
âŒ ERROR: No LLM providers configured. Please set one of:
   â€¢ Azure: AZURE_OPENAI_ENDPOINT + AZURE_OPENAI_API_KEY
   â€¢ Together AI: TOGETHER_API_KEY
```

---

#### **2. Single Provider Configured** âœ…
**Behavior**: Uses that provider exclusively (no fallback)

**Example: Azure Only**
```bash
# .env
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your-api-key
# TOGETHER_API_KEY= (not set)
```

**Result**:
```
âœ… Only Azure configured - using Azure exclusively (no fallback)
   â€¢ Active provider: AzureOpenAIProvider
   â€¢ Fallback: None
```

**Example: Together AI Only**
```bash
# .env
TOGETHER_API_KEY=your-together-key
# AZURE_OPENAI_ENDPOINT= (not set)
# AZURE_OPENAI_API_KEY= (not set)
```

**Result**:
```
âœ… Only Together AI configured - using Together exclusively (no fallback)
   â€¢ Active provider: TogetherAIProvider
   â€¢ Fallback: None
```

---

#### **3. Both Providers Configured** âœ…âœ…
**Behavior**: Smart fallback enabled

```bash
# .env
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your-azure-key
TOGETHER_API_KEY=your-together-key
```

**Result**:
```
âœ… Both Azure and Together AI configured - fallback enabled
   â€¢ Primary: Azure OpenAI (or Together AI if specified)
   â€¢ Fallback: Together AI (or Azure OpenAI)
   â€¢ Auto-switch: If primary fails
```

---

## ğŸ›ï¸ Provider Selection Modes

### **Mode 1: Auto-Detection** (Recommended)
```python
# In .env
LLM_PROVIDER=auto

# Or in code
from shared.llm import llm_client

# Will auto-detect and prefer Azure if both configured
```

**Behavior**:
- Detects configured providers from `.env`
- Prefers **Azure** if both are configured
- Falls back to **Together AI** if Azure fails
- Raises error if none configured

---

### **Mode 2: Azure Primary**
```python
# In .env
LLM_PROVIDER=azure

# Or in code
client = get_llm_client(provider_type="azure", ...)
```

**Behavior**:
- Uses Azure as primary
- Falls back to Together AI (if configured)
- No fallback if only Azure configured
- Error if Azure not configured

---

### **Mode 3: Together AI Primary**
```python
# In .env
LLM_PROVIDER=together

# Or in code
client = get_llm_client(provider_type="together", ...)
```

**Behavior**:
- Uses Together AI as primary
- Falls back to Azure (if configured)
- No fallback if only Together AI configured
- Error if Together AI not configured

---

## ğŸ“Š Configuration Matrix

| Azure | Together | Provider Type | Result |
|-------|----------|---------------|---------|
| âŒ | âŒ | auto/azure/together | âŒ **ERROR** - No providers |
| âœ… | âŒ | auto/azure | âœ… **Azure only** - No fallback |
| âœ… | âŒ | together | âŒ **ERROR** - Together not configured |
| âŒ | âœ… | auto/together | âœ… **Together only** - No fallback |
| âŒ | âœ… | azure | âŒ **ERROR** - Azure not configured |
| âœ… | âœ… | auto | âœ… **Azure primary**, Together fallback |
| âœ… | âœ… | azure | âœ… **Azure primary**, Together fallback |
| âœ… | âœ… | together | âœ… **Together primary**, Azure fallback |

---

## ğŸ› ï¸ Configuration Examples

### **Example 1: Production (Azure Only)**
```bash
# .env - Production setup with Azure only
LLM_PROVIDER=azure  # or "auto" (will detect Azure)
AZURE_OPENAI_ENDPOINT=https://prod-openai.openai.azure.com/
AZURE_OPENAI_API_KEY=prod-key-here
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4.1-mini
AZURE_OPENAI_API_VERSION=2025-04-14
```

**Behavior**: Azure exclusively, no fallback, enterprise-grade

---

### **Example 2: Development (Together AI Only)**
```bash
# .env - Dev setup with Together AI only
LLM_PROVIDER=together  # or "auto" (will detect Together)
TOGETHER_API_KEY=dev-key-here
TOGETHER_MODEL=meta-llama/Llama-3.3-70B-Instruct-Turbo
```

**Behavior**: Together AI exclusively, no fallback, cost-effective

---

### **Example 3: High Availability (Both Providers)**
```bash
# .env - HA setup with both providers
LLM_PROVIDER=auto  # or "azure" or "together"
AZURE_OPENAI_ENDPOINT=https://prod-openai.openai.azure.com/
AZURE_OPENAI_API_KEY=azure-key-here
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4.1-mini
TOGETHER_API_KEY=together-key-here
TOGETHER_MODEL=meta-llama/Llama-3.3-70B-Instruct-Turbo
```

**Behavior**: Azure primary, Together fallback, maximum reliability

---

## ğŸ§ª Testing

### **Test Current Configuration**
```bash
# Run the auto-detection test
python test_llm_provider_auto_detection.py
```

**Output**:
```
ğŸ“‹ Configured Providers:
   â€¢ Azure OpenAI:  âœ… Yes
   â€¢ Together AI:   âœ… Yes

ğŸ“Š Total Configured: 2
   âœ… Multiple providers - fallback enabled
```

---

### **Test in Code**
```python
from shared.llm_providers.factory import LLMFactory
from shared.config import settings

# Detect configured providers
configured = LLMFactory.detect_configured_providers(
    azure_endpoint=settings.azure_openai_endpoint,
    azure_api_key=settings.azure_openai_api_key,
    together_api_key=settings.together_api_key
)

print(f"Azure configured: {configured['azure']}")
print(f"Together configured: {configured['together']}")
```

---

## ğŸ”„ Fallback Flow Diagram

```
User Request
    â†“
[Auto-Detect Configured Providers]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ None        â”‚ Single         â”‚ Both        â”‚
â”‚ Configured  â”‚ Configured     â”‚ Configured  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“               â†“                â†“
  ERROR         Use Exclusively   Enable Fallback
                (No Fallback)
                    â†“                â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Azure â”‚       â”‚ Primary   â”‚
                â”‚  or   â”‚       â”‚ Provider  â”‚
                â”‚Togetherâ”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â””â”€â”€â”€â”€â”€â”€â”€â”˜            â†“
                    â†“          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“          â”‚Available?â”‚
                    â†“          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“           â†“        â†“
                    â†“         YES       NO
                    â†“          â†“         â†“
                    â†“       SUCCESS  Try Fallback
                    â†“                    â†“
                    â†“              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“              â”‚Available?â”‚
                    â†“              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“               â†“        â†“
                    â†“             YES       NO
                    â†“              â†“         â†“
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â†’ SUCCESS    ERROR
```

---

## ğŸ“ Code Examples

### **Example 1: Use Current Configuration**
```python
from shared.llm import llm_client

# Uses provider from .env (auto-detected)
response = await llm_client.chat_completion(
    messages=[{"role": "user", "content": "Hello!"}]
)
```

---

### **Example 2: Force Specific Provider**
```python
from shared.llm_providers import get_llm_client

# Force Azure (will error if not configured)
azure_client = get_llm_client(
    provider_type="azure",
    azure_endpoint="...",
    azure_api_key="..."
)

# Force Together AI (will error if not configured)
together_client = get_llm_client(
    provider_type="together",
    together_api_key="..."
)
```

---

### **Example 3: Auto-Detection with Logging**
```python
from shared.llm_providers import get_llm_client
import logging

logging.basicConfig(level=logging.INFO)

# Will log the detection process
client = get_llm_client(provider_type="auto", ...)

# Example output:
# âœ… Both Azure and Together AI configured - fallback enabled
#    â€¢ Auto-detection: Using azure as primary
#    â€¢ âœ… Azure OpenAI is available (primary)
#    â€¢ ğŸ”„ Together AI ready as fallback
```

---

## ğŸš¨ Error Messages

### **No Providers Configured**
```
âŒ No LLM providers configured. Please set one of:
   â€¢ Azure: AZURE_OPENAI_ENDPOINT + AZURE_OPENAI_API_KEY
   â€¢ Together AI: TOGETHER_API_KEY
```

**Solution**: Add credentials to `.env`

---

### **Provider Not Available**
```
âŒ Azure OpenAI configured but not available. Check credentials.
```

**Solution**: Verify API key, endpoint, and deployment name

---

### **Both Providers Failed**
```
âŒ Both Azure OpenAI and Together AI failed to initialize
```

**Solution**: Check network, credentials, and service status

---

## ğŸ¯ Best Practices

### **1. Production: Single Provider**
- Use Azure only for reliability and compliance
- No fallback complexity
- Easier to debug

```bash
LLM_PROVIDER=azure
AZURE_OPENAI_ENDPOINT=...
AZURE_OPENAI_API_KEY=...
```

---

### **2. Development: Cost-Effective**
- Use Together AI for development
- Lower costs
- Fast iteration

```bash
LLM_PROVIDER=together
TOGETHER_API_KEY=...
```

---

### **3. High Availability: Both Providers**
- Configure both for maximum uptime
- Auto-fallback on failure
- Monitor which provider is active

```bash
LLM_PROVIDER=auto
AZURE_OPENAI_ENDPOINT=...
AZURE_OPENAI_API_KEY=...
TOGETHER_API_KEY=...
```

---

## ğŸ” Troubleshooting

### **Issue: "Auto-detection using together instead of azure"**
**Cause**: Azure credentials invalid or endpoint unreachable

**Solution**:
1. Verify Azure credentials in `.env`
2. Test Azure endpoint: `curl https://your-endpoint/`
3. Check deployment name matches

---

### **Issue: "No fallback happening when primary fails"**
**Cause**: Only one provider configured

**Solution**:
```bash
# Add both providers to .env
AZURE_OPENAI_ENDPOINT=...
AZURE_OPENAI_API_KEY=...
TOGETHER_API_KEY=...
```

---

### **Issue: "Want to disable fallback"**
**Solution**: Remove one provider from `.env`

```bash
# To use Azure only (no fallback)
AZURE_OPENAI_ENDPOINT=...
AZURE_OPENAI_API_KEY=...
# TOGETHER_API_KEY= (comment out or remove)
```

---

## ğŸ“Š Monitoring

### **Check Active Provider**
```python
from shared.llm import llm_client

provider_name = type(llm_client.provider).__name__
print(f"Active: {provider_name}")
# Output: AzureOpenAIProvider or TogetherAIProvider
```

---

### **Log Provider Usage**
```python
import logging

logging.basicConfig(level=logging.INFO)

# Logs will show provider selection:
# âœ… Both Azure and Together AI configured - fallback enabled
#    â€¢ Azure OpenAI is available (primary)
```

---

## ğŸ‰ Summary

**New Capabilities**:
- âœ… Auto-detects configured providers
- âœ… Smart fallback only when multiple providers available
- âœ… Clear error messages when nothing configured
- âœ… Single provider mode (no fallback overhead)
- âœ… Multi-provider mode (high availability)
- âœ… Configuration-driven (no code changes needed)

**Migration**: No code changes required! Just update `.env` and the system adapts automatically.

---

**Last Updated**: October 22, 2025  
**Version**: 2.0 - Smart Auto-Detection
